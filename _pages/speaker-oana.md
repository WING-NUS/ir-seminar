---
layout: archive
title: "Talk Info: Oana-Maria Camburu"
permalink: /speaker-oana
author_profile: true
---

## Neural Networks with Natural Language Explanations

**Abstract**: In order for machine learning to garner widespread public adoption, models must be able to provide human-understandable and robust explanations for their decisions. In this talk, we will focus on the emerging direction of building neural networks that learn from natural language explanations at training time and generate such explanations at testing time. We will see an extension of the large Stanford Natural Language Inference (SNLI) dataset with an additional layer of human-written natural language explanations for the entailment relations, called e-SNLI. We will see different types of architectures that incorporate these explanations into their training process and generate them at testing time. We will further see a similar approach for vision-language models, where we introduce e-SNLI-VE, a large dataset of visual-textual entailment with natural language explanations. We will also see e-ViL, a benchmark for natural language explanations in vision-language tasks, and e-UG, the current SOTA model for natural language explanation generation on such tasks. These large datasets of explanations open up a range of research directions for using natural language explanations both for improving models and for asserting their trust. However, models trained on such datasets may nonetheless generate inconsistent explanations. An adversarial framework for sanity checking models over generating such inconsistencies will be presented.

**Papers Covered by this Talk**
  * [e-SNLI: Natural Language Inference with Natural Language Explanations](https://proceedings.neurips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf)
  * [e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks](https://openaccess.thecvf.com/content/ICCV2021/papers/Kayser_E-ViL_A_Dataset_and_Benchmark_for_Natural_Language_Explanations_in_ICCV_2021_paper.pdf)
  * [Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](https://aclanthology.org/2020.acl-main.382/)


## Bio

Oana-Maria Camburu is a Postdoctoral Researcher in the Department of Computer Science at the University of Oxford, from where she also received her PhD on the thesis "Explaining Deep Neural Networks". For her work on explainability, Oana received a J.P. Morgan PhD Fellowship, as well as the role of co-investigator at the Alan Turing Institute in London on the project of "Neural Networks with Natural Language Explanations". Prior to this, Oana has obtained an MSc degree in Machine Learning and an Engineering degree from the Ecole Polytechnique, Paris.

## Video Recording and Slides

**Video**
<tr>
  <td>
    <p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/-bopzFou7jQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </p>
    <p>
      <script async class="speakerdeck-embed" data-id="10ed0f26ff934922b23779e03f0d1b70" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
      </p>
  </td>
</tr>

**Slides**: [link](https://drive.google.com/file/d/1HnrbLZKU86IcC-d9OuPPvyox4gG7qKWK/view?usp=sharing)